{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8497818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "%config IPCompleter.greedy=True\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13099aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want the All_spec file or history file? (All_spec or history)  All_spec\n",
      "Do you want to get only the Current specs? (yes or no)  no\n"
     ]
    }
   ],
   "source": [
    "# what files to work\n",
    "# All_spec or history (yes)\n",
    "while True:\n",
    "    final_file_type = input(\"Do you want the All_spec file or history file? (All_spec or history)  \")\n",
    "    if final_file_type == \"All_spec\" or final_file_type == \"history\":\n",
    "        break\n",
    "# All the data or just the current\n",
    "get_current_specs = input(\"Do you want to get only the Current specs? (yes or no)  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f16a8",
   "metadata": {},
   "source": [
    "# First Steps:\n",
    "  * Uploading the data\n",
    "  * tyding up\n",
    "  * choosing the whole data or only Current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce89114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Uploading the data#\n",
    "#####################\n",
    "#\n",
    "data_source_path = \"sources/allsap.txt\"\n",
    "all_sap = pd.read_csv(data_source_path, sep='\\t')\n",
    "# Uploading Structure data ( was done manually)\n",
    "structures = pd.read_csv('sources/structures.csv')\n",
    "# getting the searcher DF \n",
    "if final_file_type == \"history\":\n",
    "    searcher = pd.read_csv(\"sources/AllAttributes.csv\")\n",
    "elif final_file_type == \"All_spec\":\n",
    "    searcher = pd.read_csv(\"sources/AllAttributes_Local.csv\")\n",
    "    \n",
    "# getting the operations df\n",
    "operations = pd.read_csv('sources/Operations.csv')\n",
    "#\n",
    "################################\n",
    "# Dropping the non useful data #\n",
    "################################\n",
    "#\n",
    "#list of columns to be dropped\n",
    "to_drop = ['Unnamed: 0','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7','Unnamed: 9','Unnamed: 11','Unnamed: 13','Unnamed: 14','MANDT','WERKS']\n",
    "all_sap.drop(to_drop, inplace=True, axis=1)\n",
    "all_sap['UDAT'] = pd.to_datetime(all_sap['UDAT'],dayfirst=True)\n",
    "all_sap = all_sap.set_index('UDAT') \n",
    "#\n",
    "###########################################\n",
    "# if you want to Get only the current spec#\n",
    "###########################################\n",
    "if get_current_specs == \"yes\":\n",
    "    rp_docs = all_sap[(all_sap[\"VALUE\"]=='RP') & \n",
    "                      (~all_sap[\"DOKNR\"].str.startswith('YA')) & \n",
    "                      (all_sap[\"CHARACT\"]== all_sap[\"CLASS\"].str[4:8] + \"_STATUS\")]['DOKNR']\n",
    "    rp_docs = set (rp_docs)\n",
    "    exclusion_list = ['SAH0561AUEG02','SAH0566AGEG03','SAH0569AAEG0D','SAH0570AFEG0M','SAH0570AGEG0M','SAH0570AIEG0A','SAH0571AJEG0B','SAH0571AKEG0A','SAH0618AEEG01','SAH0645A2EG08']\n",
    "    rp_docs = rp_docs - set(exclusion_list)\n",
    "    \n",
    "    all_sap = all_sap[all_sap[\"DOKNR\"].isin(rp_docs)]\n",
    "                      \n",
    "all_sap.to_csv('good_master_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82211eeb",
   "metadata": {},
   "source": [
    "  * Getting the missing AHHA structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b4207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHHA Structures need to be added:  []\n",
      "Classes need to be added to structures file:  []\n",
      "Classes need to be added to AllAttributes file:  []\n",
      "Do you want to continueyes\n"
     ]
    }
   ],
   "source": [
    "# find the structures missing from my file to add to structures file\n",
    "strcts_eg = all_sap[(all_sap[\"CHARACT\"]==\"AHAA_STRUCT\") & \\\n",
    "                    (all_sap[\"MATNR\"].str.contains(\"SAH\"))]['VALUE'].drop_duplicates()\n",
    "my_current_struct = structures[structures['CLASS']==\"02K6AHAA0\"]['STRUCT'].drop_duplicates()\n",
    "struct_to_add = list(set(strcts_eg)-set(my_current_struct))\n",
    "print (\"AHHA Structures need to be added: \", struct_to_add)\n",
    "\n",
    "# find the classes missing from my file to add to the structures file\n",
    "classes_eg = all_sap['CLASS'].drop_duplicates()\n",
    "my_current_classes = structures['CLASS'].drop_duplicates()\n",
    "classes_to_add = list(set(classes_eg)-set(my_current_classes))\n",
    "print (\"Classes need to be added to structures file: \", classes_to_add)\n",
    "\n",
    "\n",
    "# find the classes missing from my file to add to the ALL ATTRIBUTES file\n",
    "my_current_classes_in_AllAttributrs = searcher['COMPONENT_CLASS'].drop_duplicates()\n",
    "classes_to_add_to_AllAttributes = list(set(classes_eg)-set(my_current_classes_in_AllAttributrs))\n",
    "classes_to_add_to_AllAttributes = list(set(classes_to_add_to_AllAttributes) -  set(['0201IMAAX', '02K6LEAA0', '02K6CXAA0', '02K6ZPAC0', '02K6CRAA0', '02K6CNAD0', '02K6ZPAF0', '0203G1MEX', '02K6ZPAA0', '02K6CEBD0', '02K6CNAS0', '02K6BRAR0', '02K6CNAV0']))\n",
    "print (\"Classes need to be added to AllAttributes file: \", classes_to_add_to_AllAttributes)\n",
    "\n",
    "\n",
    "f1= input('Do you want to continue')\n",
    "if f1 == \"no\":\n",
    "    raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d51bd",
   "metadata": {},
   "source": [
    "# 1 Getting all Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568356b",
   "metadata": {},
   "source": [
    "in this file we have only one goal\n",
    "to get a DF tah contains all connections \n",
    "* SAH, SAH_DOC   SAW, SAW_DOC   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53568d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#1.1getting SAW and SAH relationship#\n",
    "#####################################\n",
    "saw_sah =all_sap[(all_sap['DOKNR'].str.contains('SAW')                    ) & \\\n",
    "                      (   (all_sap['CHARACT']=='AWAA_COMPONENT_AH') | \n",
    "                          (all_sap['CHARACT']=='C_AWAA_EG_CODICE_CORTO') | \n",
    "                          (all_sap['CHARACT']=='AWAA_RELEASE_DATE')            )\n",
    "                                         ].copy(deep=True)\n",
    "\n",
    "#droping columns \n",
    "saw_sah.drop(['ZCOUNT','CLASS','COMPONENT'], inplace=True, axis=1)\n",
    "# pivoting to create new columns for each CHARCT\n",
    "saw_sah = pd.pivot_table(saw_sah, \n",
    "                              index=['UDAT','MATNR','DOKNR'], \n",
    "                              columns='CHARACT',\n",
    "                              values='VALUE',\n",
    "                              aggfunc=lambda x: x)\n",
    "saw_sah.columns.name = None               #remove groups in columns\n",
    "saw_sah = saw_sah.reset_index()                #index to columns\n",
    "\n",
    "#renaming Some\n",
    "saw_sah.rename(columns= {'MATNR':'SAW',\n",
    "                              'DOKNR':'SAW_DOC',\n",
    "                              'AWAA_COMPONENT_AH':'SAH', \n",
    "                              'AWAA_RELEASE_DATE':'SAW_RELS_DAT',\n",
    "                              'C_AWAA_EG_CODICE_CORTO':'SAW_ShortCode'}, inplace=True)\n",
    "saw_sah.dropna(subset=['SAH'], inplace = True)\n",
    "# correcting the time format \n",
    "saw_sah['SAW_RELS_DAT'] = pd.to_datetime(saw_sah['SAW_RELS_DAT'], format='%d/%m/%Y')\n",
    "saw_sah.set_index('UDAT', inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "# #######################################################\n",
    "# #1.2getting SAH and SAH_DOC and Structure relationship#\n",
    "# #######################################################\n",
    "\n",
    "#getting the sah document structure.#\n",
    "\n",
    "sah_sahDoc = all_sap[(all_sap['MATNR'].str.contains('SAH')) &\n",
    "                             (     (all_sap['CHARACT']=='AHAA_STRUCT') |\n",
    "                                   (all_sap['CHARACT']=='AHAA_RELEASE_DATE') | \n",
    "                                   (all_sap['CHARACT']=='AHAA_RELEASE_DATE') |\n",
    "                                   (all_sap['CHARACT']=='C_AHAA_EG_CODICE_CORTO')  \n",
    "                             )].copy(deep= True)\n",
    "#droping non needed columns\n",
    "sah_sahDoc.drop(['ZCOUNT','CLASS','COMPONENT'], inplace=True, axis=1)\n",
    "# pivoting to create new columns for each CHARCT\n",
    "sah_sahDoc = pd.pivot_table(sah_sahDoc, \n",
    "                                      index=['UDAT','MATNR','DOKNR'], \n",
    "                                      columns='CHARACT',\n",
    "                                      values='VALUE',\n",
    "                                      aggfunc=lambda x: x)\n",
    "sah_sahDoc.columns.name = None               #remove groups in columns\n",
    "sah_sahDoc = sah_sahDoc.reset_index()                #index to columns\n",
    "#renaming Some\n",
    "sah_sahDoc.rename(columns= {'DOKNR':'SAH_DOC', \n",
    "                                     'MATNR':'SAH',\n",
    "                                     'AHAA_RELEASE_DATE':'SAH_RELS_DAT', \n",
    "                                     'C_AHAA_EG_CODICE_CORTO':'SAH_ShortCode'}, inplace=True)\n",
    "# # correcting the time format \n",
    "sah_sahDoc['SAH_RELS_DAT'] = pd.to_datetime(sah_sahDoc['SAH_RELS_DAT'], format='%d/%m/%Y')\n",
    "sah_sahDoc.set_index('UDAT', inplace= True)\n",
    "\n",
    "# ################################################\n",
    "# # 1.3 Getting SAH_DOC, SAW_DOC and their dates #\n",
    "# ################################################\n",
    "sahs = sah_sahDoc.copy(deep = True)\n",
    "sahs.reset_index( inplace= True)\n",
    "sahs.rename (columns = {'UDAT':'SAH_DAT'}, inplace= True)\n",
    "sahs.drop(columns=['AHAA_STRUCT'], inplace = True)\n",
    "\n",
    "saws = saw_sah.copy(deep = True)\n",
    "saws.reset_index( inplace= True)\n",
    "saws.rename (columns = {'UDAT':'SAW_DAT'}, inplace= True)\n",
    "# saws.drop(columns=['SAH','AHAA_STRUCT'], inplace = True)\n",
    "asma = pd.merge(saws, sahs, left_on=['SAH','SAW_ShortCode'], right_on=['SAH','SAH_ShortCode'], how = 'inner')\n",
    "asma.drop_duplicates('SAH_DOC', keep='last', inplace = True )\n",
    "asma.drop(columns=['SAW_ShortCode','SAH_ShortCode'], axis=1 , inplace= True)\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "# 1.4 Getting the Date of SAW_DOC termination #\n",
    "###############################################\n",
    "\n",
    "\n",
    "# getting the production status for each saw doc\n",
    "unique_SAW_DOC = pd.DataFrame(saw_sah['SAW_DOC'])\n",
    "unique_SAW_DOC['Indu_Status_helper'] = \"AWAA_STATUS\"\n",
    "SAW_DOC_STATE = pd.merge(unique_SAW_DOC, all_sap, \n",
    "                         left_on=['SAW_DOC','Indu_Status_helper'], \n",
    "                         right_on = ['DOKNR','CHARACT'], how = 'left')\n",
    "SAW_DOC_STATE.drop(['Indu_Status_helper','MATNR','DOKNR','ZCOUNT','CLASS','CHARACT','COMPONENT'], \n",
    "                   inplace = True, axis=1)\n",
    "\n",
    "# getting the release date for all saw doc\n",
    "SAW_DOC_STATE_RelsDate = pd.merge(SAW_DOC_STATE,saw_sah[['SAW_DOC','SAW_RELS_DAT']], on= 'SAW_DOC')\n",
    "\n",
    "\n",
    "\n",
    "#Changing the date of only RP saw doc to be the current date\n",
    "# Get file's Last modification time stamp only in terms of seconds since epoch \n",
    "modTimesinceEpoc = os.path.getctime(data_source_path)\n",
    "# Convert seconds since epoch to readable timestamp\n",
    "# extractionTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modTimesinceEpoc))\n",
    "extractionTime = time.strftime('%d-%m-%y %H:%M:%S', time.localtime(modTimesinceEpoc))\n",
    "# print(\"Last Modified Time : \", modificationTime )\n",
    "\n",
    "SAW_DOC_STATE_RelsDate.loc[SAW_DOC_STATE_RelsDate['VALUE'] == 'RP', 'SAW_RELS_DAT'] = extractionTime\n",
    "# SAW_DOC_STATE_RelsDate['SAW_RELS_DAT'] = SAW_DOC_STATE_RelsDate['SAW_RELS_DAT'].dt.strftime('%Y-%m-%d')\n",
    "SAW_DOC_STATE_RelsDate.set_index('SAW_DOC', inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# doing the same of SAH (changing the dates of RP SAH to be as extraction time)\n",
    "# the list of SAH that are currently RP\n",
    "sahDoc_RP = all_sap[(all_sap['MATNR'].str.contains('SAH')) &((all_sap['CHARACT']=='AHAA_STATUS'))&((all_sap['VALUE']=='RP'))\n",
    "                   ]['DOKNR'].copy(deep= True)\n",
    "asma.loc[asma['SAH_DOC'].isin(sahDoc_RP),'SAH_DAT'] = extractionTime\n",
    "asma['SAH_DAT'] = asma['SAH_DAT'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6946c",
   "metadata": {},
   "source": [
    "# 2 Getting all the SC levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd544d",
   "metadata": {},
   "source": [
    "in this file we have only one goal\n",
    "to get a DF tah contains \n",
    "* SAH_DOC\n",
    "* All_COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37df3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# 2.1 getting date, SAH and SAH_Components (the first level)#\n",
    "#############################################################\n",
    "## 2.1.1 adding the data from SAH_Doc to know the Class of each Semicomponent and the SC themselves\n",
    "sah_sahDoc_classes = pd.merge(sah_sahDoc, structures, left_on='AHAA_STRUCT', right_on='STRUCT',how = 'left')\n",
    "sah_sahDoc_classes.drop(['SAH','CLASS','STRUCT','CHARCT_toGetSublen','CHARCT_toGetSubWeight'], inplace= True, axis =1 )\n",
    "#might need something from above to try and if no keep it\n",
    "\n",
    "## 2.1.2 getting the Semi-Component documents using CHARCT_toGetSubDoc \n",
    "sah_sahDoc_classes_compDoc = pd.merge(sah_sahDoc_classes,all_sap, left_on = ['SAH_DOC','CHARCT_toGetSubDoc'],\n",
    "                                     right_on = ['DOKNR','CHARACT'])\n",
    "##dropping \n",
    "sah_sahDoc_classes_compDoc.drop(['CLASS','MATNR','DOKNR','ZCOUNT','COMPONENT','AHAA_STRUCT','SUB_EF','CHARCT_toGetSubDoc','CHARACT','SAH_ShortCode'], axis=1 , inplace = True)\n",
    "sah_sahDoc_classes_compDoc.rename(columns={'VALUE':'COMPONENT_DOC','SUB_CLASSES':'COMPONENT_CLASS'}, inplace = True)\n",
    "\n",
    "#######################################################\n",
    "# 2.2 to get the sub sub components (the second level)#\n",
    "#######################################################\n",
    "\n",
    "suba = sah_sahDoc_classes_compDoc.copy(deep = True)\n",
    "subs = sah_sahDoc_classes_compDoc.copy(deep = True)\n",
    "for i in range(0, 10):\n",
    "    suba = pd.merge(suba, \n",
    "                   structures[['CLASS','CHARCT_toGetSubDoc','SUB_CLASSES']], \n",
    "                   left_on=['COMPONENT_CLASS'], right_on=['CLASS'], how='left')\n",
    "    suba.drop(['COMPONENT_CLASS','CLASS'], inplace= True, axis=1)\n",
    "\n",
    "    # 2.2.2 getting the S-C documents using CHARCT_toGetSubDoc \n",
    "    suba = pd.merge(suba, all_sap[['DOKNR','CHARACT','VALUE']], \n",
    "                       left_on=['COMPONENT_DOC','CHARCT_toGetSubDoc'], right_on= ['DOKNR','CHARACT'], how='left')\n",
    "    ## sub_sub = sub_sub.dropna()\n",
    "    suba.drop(columns={'CHARCT_toGetSubDoc','DOKNR','CHARACT','COMPONENT_DOC'}, inplace= True)\n",
    "    suba.rename(columns={'SUB_CLASSES':'COMPONENT_CLASS','VALUE':'COMPONENT_DOC'}, inplace = True)\n",
    "    suba.dropna(subset = ['COMPONENT_DOC'], inplace = True)\n",
    "    suba.drop_duplicates(subset=[\"SAH_DOC\",\"COMPONENT_NAME\",\"COMPONENT_CLASS\",\"COMPONENT_DOC\"],inplace = True)\n",
    "    \n",
    "    \n",
    "    subs = pd.concat(\n",
    "    [subs,suba],\n",
    "    axis=0,\n",
    "    join=\"outer\",\n",
    "    ignore_index=False,\n",
    "    keys=None,\n",
    "    levels=None,\n",
    "    names=None,\n",
    "    verify_integrity=False,\n",
    "    copy=True)\n",
    "\n",
    "subs.drop_duplicates(inplace = True)\n",
    "\n",
    "######################\n",
    "# 2.3 adding SAW lev #\n",
    "######################\n",
    "sawDoc_as_component = asma[['SAH_DOC','SAW_DOC','SAH_RELS_DAT']].copy(deep= True)\n",
    "# renaming columns to mach the ones of SAH_Components df\n",
    "sawDoc_as_component.rename(columns={'SAW_DOC':'COMPONENT_DOC'}, inplace = True)\n",
    "sawDoc_as_component['COMPONENT_CLASS'] = '02K6AWAA0'\n",
    "sawDoc_as_component['COMPONENT_NAME']= \"IP\"\n",
    "\n",
    "###########################################\n",
    "# 2.4 adding SAH_DOC in the somponent col # \n",
    "###########################################\n",
    "\n",
    "SahDoc_as_component = sah_sahDoc_classes_compDoc.drop_duplicates('SAH_DOC').copy(deep = True )\n",
    "SahDoc_as_component['COMPONENT_DOC']=SahDoc_as_component['SAH_DOC']\n",
    "SahDoc_as_component['COMPONENT_CLASS']='02K6AHAA0'\n",
    "SahDoc_as_component['COMPONENT_NAME']= \"Central\"\n",
    "\n",
    "\n",
    "#####################\n",
    "# 2.5 Concatinating # \n",
    "#####################\n",
    "\n",
    "# 2.5.1 The concatination\n",
    "sahDoc_Comp = pd.concat(\n",
    "    [sah_sahDoc_classes_compDoc,subs,sawDoc_as_component,SahDoc_as_component],\n",
    "    axis=0,\n",
    "    join=\"outer\",\n",
    "    ignore_index=False,\n",
    "    keys=None,\n",
    "    levels=None,\n",
    "    names=None,\n",
    "    verify_integrity=False,\n",
    "    copy=True)\n",
    "\n",
    "# 2.5.2 to add the UDAT (not sure if needed)\n",
    "s = sah_sahDoc.copy(deep=True)\n",
    "s.reset_index(inplace=True)\n",
    "sahDoc_Comp = pd.merge(sahDoc_Comp, s[['UDAT','SAH_DOC']], on= 'SAH_DOC', how='left')\n",
    "sahDoc_Comp = sahDoc_Comp.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75506258",
   "metadata": {},
   "source": [
    "# 3 The Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4d5f6",
   "metadata": {},
   "source": [
    "Result DF: \n",
    "  * SAH_DOC\n",
    "  * Col_index\n",
    "  * VALUE\n",
    "  * col_name: might not need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2c1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "# 3.1 Merging all_sap with searcher (the list of attrubutes and their place in all_spec)#\n",
    "#########################################################################################\n",
    "all_sap1= all_sap.copy(deep = True)\n",
    "all_sap1.drop(columns=['ZCOUNT','COMPONENT','MATNR'], inplace= True,axis=1)\n",
    "#renaming so we have only one column after merging\n",
    "all_sap1.rename(columns={'CLASS':'COMPONENT_CLASS','DOKNR':'COMPONENT_DOC','CHARACT':'TECH_NAME'} , inplace = True)\n",
    "# merging all_sap with searcher to get results DF \n",
    "data_id  = pd.merge(sahDoc_Comp,searcher, on=['COMPONENT_CLASS','COMPONENT_NAME'], how='right')\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 3.2 merging the data_id with sahDoc_Comp to get full_data #\n",
    "#############################################################\n",
    "#we need to get\n",
    "full_data = pd.merge(data_id, all_sap1, left_on=['COMPONENT_DOC','COMPONENT_CLASS','TECH_NAME'], \n",
    "                   right_on=['COMPONENT_DOC', 'COMPONENT_CLASS','TECH_NAME'], how = 'right')\n",
    "full_data.dropna(inplace= True) ##############################################################################?\n",
    "\n",
    "full_data.set_index('UDAT', inplace= True)\n",
    "full_data.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130499f0",
   "metadata": {},
   "source": [
    "# 4) The arranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b80d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_current_specs == 'yes':\n",
    "    # straight to the pivot\n",
    "    resultsLocal = full_data.copy(deep =True)\n",
    "    resultsLocal.reset_index(inplace = True)\n",
    "    resultsLocal.sort_values(['SAH_DOC','col_index'], inplace= True)\n",
    "    resultsLocal = resultsLocal[resultsLocal['col_index'] != -1]   \n",
    "    \n",
    "    hey = pd.pivot_table(resultsLocal,index=['SAH_DOC'],columns='col_index',values='VALUE',\n",
    "                         aggfunc='first')    \n",
    "\n",
    "    \n",
    "else:\n",
    "\n",
    "    #######################\n",
    "    # 4.1 Getting mondays #\n",
    "    #######################\n",
    "\n",
    "    # Getting all Mondays in years ..\n",
    "    if get_current_specs == 'yes':\n",
    "        my_start = date.today() - timedelta(7)\n",
    "    else:\n",
    "        my_start = '2008-01-01'\n",
    "    my_end = pd.to_datetime(\"today\")\n",
    "    mondays = pd.date_range(start = my_start ,end = my_end , freq = \"W-MON\")\n",
    "\n",
    "    # putting them in a new df\n",
    "    one_column_df = pd.DataFrame(mondays)\n",
    "    one_column_df.rename(columns = {0:'Date'}, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    ################\n",
    "    # 4.2 the loop #\n",
    "    ################\n",
    "    weeks_sawDoc_sahDoc_list = []\n",
    "\n",
    "    # might need to convert back to SAW rather than SAW_DOC or use the SAW_DAT for both start and end\n",
    "    for sawDoc in saw_sah['SAW_DOC'].unique():\n",
    "        all_version_df = asma[asma['SAW_DOC']== sawDoc]\n",
    "        #getting start and end date of the SAW document\n",
    "        t0a = all_version_df['SAW_RELS_DAT'].min()\n",
    "        t0b = all_version_df['SAW_DAT'].min()\n",
    "        t0c = all_version_df['SAH_RELS_DAT'].min()\n",
    "        t0d = all_version_df['SAH_DAT'].min()\n",
    "        saw_start = min([t0a, t0b, t0c, t0d])\n",
    "\n",
    "\n",
    "        saw_last = SAW_DOC_STATE_RelsDate.loc[sawDoc,'SAW_RELS_DAT']\n",
    "\n",
    "        #greater than the start date and smaller than the end date\n",
    "        mask = (one_column_df['Date'] > saw_start) & (one_column_df['Date'] <= saw_last)\n",
    "\n",
    "        timeFrameDf = one_column_df.loc[mask]\n",
    "\n",
    "        # merging and filling the weeks\n",
    "        timeWithSah = pd.merge_asof (timeFrameDf, \n",
    "                                     all_version_df.sort_values('SAH_DAT'), \n",
    "                                     left_on= ['Date'], right_on=['SAH_DAT'], direction='forward')\n",
    "        timeWithSah= timeWithSah[['Date','SAW_DOC','SAH_DOC']]\n",
    "\n",
    "        #add to master list\n",
    "        weeks_sawDoc_sahDoc_list.append(timeWithSah)\n",
    "\n",
    "\n",
    "    weeks_sawDoc_sahDoc_df = pd.concat(weeks_sawDoc_sahDoc_list)\n",
    "\n",
    "\n",
    "    #################################################################\n",
    "    # 4.3 Merging the data with required dates (a sample each week) # \n",
    "    #################################################################\n",
    "\n",
    "    weeks_sawDoc_sahDoc_df.dropna( inplace= True)\n",
    "    resultsWweeks = pd.merge(weeks_sawDoc_sahDoc_df,full_data, on=['SAH_DOC'], how ='left' )\n",
    "    resultsWweeks.sort_values(['Date','SAW_DOC','SAH_DOC','col_index'], inplace= True)\n",
    "    resultsWweeks = resultsWweeks[resultsWweeks['col_index'] != -1]\n",
    "\n",
    "    ####################\n",
    "    # 4.4 Re-arranging #\n",
    "    ####################\n",
    "    hey = pd.pivot_table(resultsWweeks,index=['Date','SAW_DOC','SAH_DOC'],columns='col_index',values='VALUE',\n",
    "                   aggfunc='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd3468fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# 4.5 Final tuning #\n",
    "####################\n",
    "\n",
    "#finding the missing columns\n",
    "# all columns there is\n",
    "all_cols = searcher[['col_index']].copy(deep = True)\n",
    "all_cols.drop_duplicates(inplace = True)\n",
    "all_cols = all_cols[all_cols['col_index'] != -1]\n",
    "all_cols_list = list(all_cols['col_index'])\n",
    "# the columns present in the final DF\n",
    "current_list = list(hey.columns)\n",
    "# missing cols\n",
    "missing_cols = list(set(all_cols_list) - set(current_list))\n",
    "\n",
    "hey[missing_cols]= \"\"\n",
    "salma = hey.reindex(sorted(hey.columns), axis=1)\n",
    "\n",
    "\n",
    "# firist drop the index \n",
    "salma.reset_index (inplace = True)\n",
    "\n",
    "if final_file_type == \"history\":\n",
    "    # getting only the results I need 2018 onwards\n",
    "#     salma = salma[(salma['Date']>='2017-12-31')]\n",
    "    salma = salma[(salma['Date']>='2021-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e19c12",
   "metadata": {},
   "source": [
    "# 5) Manual shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd983c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_file_type == \"history\":\n",
    "    # Plant col\n",
    "    salma.loc[:,1] = 'M001'\n",
    "    # Date col\n",
    "    salma.loc[:,2] = salma.loc[:,'Date']\n",
    "    # Week col \n",
    "    salma.loc[:,3] = salma.loc[:,'Date'].dt.isocalendar().week\n",
    "    # month name col\n",
    "    salma.loc[:,4] = salma.loc[:,'Date'].dt.month_name()\n",
    "    # year name\n",
    "    salma.loc[:,5] = salma.loc[:,'Date'].dt.year\n",
    "    # IPCODE/Description\n",
    "    salma.loc[:,7] = salma.loc[:,9].str.cat(salma.loc[:,11], sep =\"/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Converting commas to dots\n",
    "\n",
    "    cols_to_convert = list(set(operations[operations['is_numer']=='Yes']['col_index']))\n",
    "    # 1 replace all \".\"\" with nothing (regex false or true dont know)\n",
    "    salma.loc[:, (cols_to_convert)] = salma.loc[:, (cols_to_convert)].apply(lambda src: src.str.replace('.', '', regex = True))\n",
    "    # 2 replace all the \",\" with \".\" (regex false or true dont know)\n",
    "    salma.loc[:, (cols_to_convert)] = salma.loc[:, (cols_to_convert)].apply(lambda src: src.str.replace(',', '.', regex = True))\n",
    "    # 3 do the operation\n",
    "    salma.loc[:, (cols_to_convert)] = salma.loc[:, (cols_to_convert)].astype(float, errors = 'ignore')\n",
    "    # salma.loc[:,(cols_to_convert)]\n",
    "\n",
    "\n",
    "\n",
    "    #Making the operations\n",
    "    # dividing operations\n",
    "    # the rows where the operation is required only\n",
    "    to_operate_onDF = operations[(operations['needs_work']==\"YES\") &(operations['Operation_type']==\"/\")].copy(deep = True)\n",
    "    # The list of columns that needs operations\n",
    "    cols_to_operate = list(set(to_operate_onDF['col_index']))\n",
    "    # Setting the col index as index\n",
    "    to_operate_onDF.set_index('col_index', inplace = True)\n",
    "    # to convers the columns needing operations to floats in the big df\n",
    "    salma.loc[:, (cols_to_operate)] = salma.loc[:, (cols_to_operate)].astype(float, errors = 'ignore')\n",
    "    # Converting the column of factor to float\n",
    "    to_operate_onDF.loc[:,'factor'] = to_operate_onDF.loc[:,'factor'].astype(float, errors = 'ignore')\n",
    "\n",
    "    #The operation\n",
    "    salma.loc[:, cols_to_operate] = salma.loc[:, cols_to_operate]/ to_operate_onDF.T.loc['factor',cols_to_operate]\n",
    "\n",
    "\n",
    "\n",
    "    # total SideWall Volume\n",
    "\n",
    "\n",
    "    #droping the columns not needed\n",
    "    salma.drop( columns = ['Date','SAW_DOC','SAH_DOC'], inplace = True)\n",
    "    #renaming the col index\n",
    "\n",
    "    col_names_index_df = searcher[['col_index','COMPONENT_ATTRIB']]\n",
    "    col_names_index_df = col_names_index_df.copy(deep = True)\n",
    "    col_names_index_df.drop_duplicates(subset=(\"col_index\"),inplace = True)\n",
    "    salma.rename(columns=dict(zip(col_names_index_df[\"col_index\"], col_names_index_df[\"COMPONENT_ATTRIB\"])), inplace = True)\n",
    "    #setting plant as index instead of col_index\n",
    "    salma.set_index( 'PLANT' , inplace = True)\n",
    "    salma.to_excel('All_spec.xlsx')\n",
    "\n",
    "    \n",
    "    \n",
    "elif final_file_type == \"All_spec\":\n",
    "    \n",
    "    #droping the columns not needed\n",
    "    salma.drop( columns = ['SAH_DOC'], inplace = True)\n",
    "    \n",
    "    #renaming the col index\n",
    "    col_names_index_df = searcher[['col_index','COMPONENT_ATTRIB']]\n",
    "    col_names_index_df = col_names_index_df.copy(deep = True)\n",
    "    col_names_index_df.drop_duplicates(subset=(\"col_index\"),inplace = True)\n",
    "    salma.rename(columns=dict(zip(col_names_index_df[\"col_index\"], col_names_index_df[\"COMPONENT_ATTRIB\"])), inplace = True)\n",
    "    \n",
    "    # Stupid solution to the SAH doc that are falsly appear as RP while they are not and not all of them\n",
    "    salma.dropna(subset = ['SAP Curing Code'], inplace = True)\n",
    "    \n",
    "    salma.to_excel('All_spec_local.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce550b",
   "metadata": {},
   "source": [
    "# Checking \n",
    "Using the History approach "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
